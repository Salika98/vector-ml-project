{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Part1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.6.9 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
      ],
      "outputs": [],
      "metadata": {
        "id": "GIk4Nn3X-3k3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "### Those parameters could be changed, depending on the dataset and the copmutation ###\n",
        "# Number of different classes/labels \n",
        "num_classes = 10\n",
        "\n",
        "# Number of image colour channels. 1 for gray scale, 3 for RGB\n",
        "colour_channels = 1\n",
        "\n",
        "# Parameters for training the model\n",
        "batch_size = 128\n",
        "epochs = 5"
      ],
      "outputs": [],
      "metadata": {
        "id": "VOrUacBUlK0S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "### To use different datasets, the following format must be followed:\n",
        "### x_train_orig, x_test_orig - (number of samples, image rows, image cols)\n",
        "### y_train_orig, y_test_orig - (number of samples,)\n",
        "\n",
        "# Load the dataset\n",
        "(x_train_orig, y_train_orig), (x_test_orig, y_test_orig) = fashion_mnist.load_data()"
      ],
      "outputs": [],
      "metadata": {
        "id": "A-ETdVSW-86x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Example of a picture in gray scale\n",
        "index = 1\n",
        "plt.imshow(x_train_orig[index], cmap='gray')\n",
        "print (\"y = \" + str(np.squeeze(y_train_orig[index])))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "zPtuGPxmFL6B",
        "outputId": "988be4ec-4cc2-4ef1-d060-e1c25c006028"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# Number of testing and training examples\n",
        "m_train = x_train_orig.shape[0]\n",
        "m_test = x_test_orig.shape[0]\n",
        "\n",
        "# Number of rows and cols in an image\n",
        "img_rows = x_train_orig.shape[1]\n",
        "img_cols = x_train_orig.shape[2]\n",
        "\n",
        "# Reshape\n",
        "x_train = x_train_orig.reshape(x_train_orig.shape[0], img_rows, img_cols, colour_channels)\n",
        "x_test = x_test_orig.reshape(x_test_orig.shape[0], img_rows, img_cols, colour_channels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "tcrpvQnyijl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize image vectors\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "outputs": [],
      "metadata": {
        "id": "DICcg6SbihiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train_orig, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test_orig, num_classes)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ffedTzCPkH4m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# Shape of an image\n",
        "input_shape = (img_rows, img_cols, colour_channels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "0K2oJl7VtYA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "print (\"Number of training examples: \" + str(m_train))\n",
        "print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: \" + str(input_shape))\n",
        "print (\"x_train shape: \" + str(x_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"x_test shape: \" + str(x_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 60000\n",
            "Number of testing examples: 10000\n",
            "Each image is of size: (28, 28, 1)\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000, 10)\n",
            "x_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000, 10)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hmtc9LO_Sbm",
        "outputId": "d1238ede-703e-4ffc-ff4d-29d41dd9fd18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "# Define CNN model\n",
        "def model(input_shape):\n",
        "    \"\"\"\n",
        "    Implementation of a CNN for image classification.\n",
        "    \n",
        "    Arguments:\n",
        "    input_shape - shape of the images of the dataset\n",
        "\n",
        "    Returns:\n",
        "    model - a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                    activation='relu',\n",
        "                    input_shape=input_shape))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "outputs": [],
      "metadata": {
        "id": "9SjDRzrdJMPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "# Create the model\n",
        "cnn_image_model = model(input_shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "JSbPc-bvaGTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "# Compile model\n",
        "cnn_image_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Nadam(),metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "t__SE3DFXRPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "# Train model\n",
        "cnn_image_model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.5168 - acc: 0.8168 - val_loss: 0.3304 - val_acc: 0.8775\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.3370 - acc: 0.8809 - val_loss: 0.2846 - val_acc: 0.8982\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.2914 - acc: 0.8957 - val_loss: 0.2606 - val_acc: 0.9055\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 83s 1ms/sample - loss: 0.2596 - acc: 0.9054 - val_loss: 0.2467 - val_acc: 0.9107\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 84s 1ms/sample - loss: 0.2348 - acc: 0.9125 - val_loss: 0.2390 - val_acc: 0.9135\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6081c3f5c0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bgEQttRbqbf",
        "outputId": "c87743f4-2d0d-4431-d66e-d4483581e8f0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "# Evaluate model\n",
        "score = cnn_image_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.2389760017454624\n",
            "Test accuracy: 0.9135\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI2ZiSFYmApK",
        "outputId": "1a16289a-ead4-4dca-87ee-0f034160d0ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "source": [
        "# Save the entire model\n",
        "cnn_image_model.save('part3/models/cnn_images_model')"
      ],
      "outputs": [],
      "metadata": {}
    }
  ]
}